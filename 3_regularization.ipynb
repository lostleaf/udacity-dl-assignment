{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n",
      "(10000, 784)\n",
      "(10000, 784)\n",
      "(200000,) (200000, 784) (200000, 10)\n",
      "(10000,) (10000, 784) (10000, 10)\n",
      "(10000,) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(valid_dataset.shape)\n",
    "print(test_dataset.shape)\n",
    "\n",
    "train_dataset.flags.writeable = False\n",
    "valid_dataset.flags.writeable = False\n",
    "test_dataset.flags.writeable = False\n",
    "\n",
    "from itertools import izip\n",
    "\n",
    "data_label = dict()\n",
    "\n",
    "def add_dataset(dataset, labels):\n",
    "    for data, label in izip(dataset, labels):\n",
    "        if data.data not in data_label:\n",
    "            data_label[data.data] = set()\n",
    "        data_label[data.data].add(label)\n",
    "        \n",
    "add_dataset(train_dataset, save[\"train_labels\"])\n",
    "add_dataset(valid_dataset, save[\"valid_labels\"])\n",
    "add_dataset(test_dataset, save[\"test_labels\"])\n",
    "\n",
    "ambiguous_data = set(data for data, label_set in data_label.iteritems() if len(label_set) > 1)\n",
    "\n",
    "occurred = set()\n",
    "# remove ambiguous and deduplicate\n",
    "def cleaned(dataset, labels):\n",
    "    mask = []\n",
    "    for data in dataset:\n",
    "        mask.append(data.data not in ambiguous_data and data.data not in occurred)\n",
    "        occurred.add(data.data)\n",
    "    mask = np.array(mask)\n",
    "    print(mask.shape, dataset.shape, labels.shape)\n",
    "    return dataset[mask, :], labels[mask, :]\n",
    "\n",
    "train_dataset, train_labels = cleaned(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = cleaned(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = cleaned(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.625202\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 500: 2.670782\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 1000: 1.733475\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 1500: 1.201692\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 2000: 1.070226\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 2500: 0.946858\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 3000: 0.841304\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.2%\n",
      "Test accuracy: 87.8%\n"
     ]
    }
   ],
   "source": [
    "# LR + L2 regularization\n",
    "beta = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "# Input data. For the training data, we use a placeholder that will be fed\n",
    "# at run time with a training minibatch.\n",
    "tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                shape=(batch_size, image_size * image_size))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "# Variables.\n",
    "weights = tf.Variable(\n",
    "tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "# Training computation.\n",
    "logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "loss = loss + tf.reduce_mean(tf.nn.l2_loss(weights)) * beta\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "# Predictions for the training, validation, and test data.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "valid_prediction = tf.nn.softmax(\n",
    "tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "minibatch loss 661.489014\n",
      "minibatch accuracy 0.109375\n",
      "validation accuracy 0.181380\n",
      "step 2000\n",
      "minibatch loss 213.647110\n",
      "minibatch accuracy 0.820312\n",
      "validation accuracy 0.809769\n",
      "step 4000\n",
      "minibatch loss 139.746170\n",
      "minibatch accuracy 0.851562\n",
      "validation accuracy 0.824670\n",
      "step 6000\n",
      "minibatch loss 92.858833\n",
      "minibatch accuracy 0.929688\n",
      "validation accuracy 0.823885\n",
      "step 8000\n",
      "minibatch loss 62.133446\n",
      "minibatch accuracy 0.906250\n",
      "validation accuracy 0.833968\n",
      "step 10000\n",
      "minibatch loss 41.858868\n",
      "minibatch accuracy 0.867188\n",
      "validation accuracy 0.851781\n",
      "step 12000\n",
      "minibatch loss 28.116917\n",
      "minibatch accuracy 0.867188\n",
      "validation accuracy 0.854246\n",
      "step 14000\n",
      "minibatch loss 18.785597\n",
      "minibatch accuracy 0.929688\n",
      "validation accuracy 0.863433\n",
      "step 16000\n",
      "minibatch loss 12.837034\n",
      "minibatch accuracy 0.882812\n",
      "validation accuracy 0.872731\n",
      "step 18000\n",
      "minibatch loss 8.763467\n",
      "minibatch accuracy 0.898438\n",
      "validation accuracy 0.874300\n",
      "step 20000\n",
      "minibatch loss 5.945143\n",
      "minibatch accuracy 0.890625\n",
      "validation accuracy 0.880798\n",
      "step 22000\n",
      "minibatch loss 4.100549\n",
      "minibatch accuracy 0.921875\n",
      "validation accuracy 0.883935\n",
      "step 24000\n",
      "minibatch loss 2.916426\n",
      "minibatch accuracy 0.921875\n",
      "validation accuracy 0.886960\n",
      "step 26000\n",
      "minibatch loss 2.084231\n",
      "minibatch accuracy 0.929688\n",
      "validation accuracy 0.890433\n",
      "step 28000\n",
      "minibatch loss 1.554688\n",
      "minibatch accuracy 0.921875\n",
      "validation accuracy 0.889648\n",
      "step 30000\n",
      "minibatch loss 1.200117\n",
      "minibatch accuracy 0.921875\n",
      "validation accuracy 0.892225\n",
      "0.947148\n"
     ]
    }
   ],
   "source": [
    "#fully connected NN + L2\n",
    "batch_size = 128\n",
    "num_steps = 30000 + 1\n",
    "beta = 0.001\n",
    "\n",
    "#input data and labels\n",
    "x = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "#hidden layer\n",
    "W_1 = tf.Variable(tf.truncated_normal([image_size * image_size, 1024]))\n",
    "b_1 = tf.Variable(tf.zeros([1024]))\n",
    "h_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "#output layer\n",
    "W_2 = tf.Variable(tf.truncated_normal([1024, num_labels]))\n",
    "b_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "logits = tf.matmul(h_1, W_2) + b_2\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_true))\n",
    "l2 = beta * (tf.reduce_mean(tf.nn.l2_loss(W_1)) + tf.reduce_mean(tf.nn.l2_loss(W_2)))\n",
    "loss = loss + l2\n",
    "\n",
    "#optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "#accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(prob, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        _, l, a = sess.run([optimizer, loss, accuracy], feed_dict={x : batch_data, y_true : batch_labels})\n",
    "        if step % 2000 == 0:\n",
    "            print(\"step %d\" % step)\n",
    "            print(\"minibatch loss %f\" % l)\n",
    "            print(\"minibatch accuracy %f\" % a)\n",
    "            a = sess.run(accuracy, feed_dict={x: valid_dataset, y_true: valid_labels})\n",
    "            print(\"validation accuracy %f\" % a)\n",
    "        \n",
    "    print(sess.run(accuracy, feed_dict={x: test_dataset, y_true: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "minibatch loss 754.092773\n",
      "minibatch accuracy 0.101562\n",
      "validation accuracy 0.233923\n",
      "step 200\n",
      "minibatch loss 301.571777\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.532377\n",
      "step 400\n",
      "minibatch loss 289.746490\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.532489\n",
      "step 600\n",
      "minibatch loss 278.384827\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.532489\n",
      "step 800\n",
      "minibatch loss 267.468628\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.532489\n",
      "step 1000\n",
      "minibatch loss 256.980469\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.532601\n",
      "0.58929\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000 + 1\n",
    "batch_data = train_dataset[:batch_size, :]\n",
    "batch_labels = train_labels[:batch_size, :]\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "\n",
    "        _, l, a = sess.run([optimizer, loss, accuracy], feed_dict={x : batch_data, y_true : batch_labels})\n",
    "        if step % 200 == 0:\n",
    "            print(\"step %d\" % step)\n",
    "            print(\"minibatch loss %f\" % l)\n",
    "            print(\"minibatch accuracy %f\" % a)\n",
    "            a = sess.run(accuracy, feed_dict={x: valid_dataset, y_true: valid_labels})\n",
    "            print(\"validation accuracy %f\" % a)\n",
    "        \n",
    "    print(sess.run(accuracy, feed_dict={x: test_dataset, y_true: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "minibatch loss 846.167236\n",
      "minibatch accuracy 0.140625\n",
      "validation accuracy 0.213421\n",
      "step 5000\n",
      "minibatch loss 113.749832\n",
      "minibatch accuracy 0.773438\n",
      "validation accuracy 0.806408\n",
      "step 10000\n",
      "minibatch loss 42.011421\n",
      "minibatch accuracy 0.804688\n",
      "validation accuracy 0.836881\n",
      "step 15000\n",
      "minibatch loss 15.589698\n",
      "minibatch accuracy 0.851562\n",
      "validation accuracy 0.854246\n",
      "step 20000\n",
      "minibatch loss 5.999804\n",
      "minibatch accuracy 0.867188\n",
      "validation accuracy 0.866906\n",
      "step 25000\n",
      "minibatch loss 2.494871\n",
      "minibatch accuracy 0.875000\n",
      "validation accuracy 0.875196\n",
      "step 30000\n",
      "minibatch loss 1.219657\n",
      "minibatch accuracy 0.906250\n",
      "validation accuracy 0.881806\n",
      "step 35000\n",
      "minibatch loss 0.680283\n",
      "minibatch accuracy 0.929688\n",
      "validation accuracy 0.884943\n",
      "step 40000\n",
      "minibatch loss 0.474037\n",
      "minibatch accuracy 0.937500\n",
      "validation accuracy 0.884495\n",
      "step 45000\n",
      "minibatch loss 0.617348\n",
      "minibatch accuracy 0.875000\n",
      "validation accuracy 0.884831\n",
      "step 50000\n",
      "minibatch loss 0.489791\n",
      "minibatch accuracy 0.898438\n",
      "validation accuracy 0.884943\n",
      "step 55000\n",
      "minibatch loss 0.409979\n",
      "minibatch accuracy 0.906250\n",
      "validation accuracy 0.888304\n",
      "step 60000\n",
      "minibatch loss 0.551438\n",
      "minibatch accuracy 0.867188\n",
      "validation accuracy 0.887968\n",
      "step 65000\n",
      "minibatch loss 0.554328\n",
      "minibatch accuracy 0.890625\n",
      "validation accuracy 0.888416\n",
      "step 70000\n",
      "minibatch loss 0.455790\n",
      "minibatch accuracy 0.898438\n",
      "validation accuracy 0.888752\n",
      "step 75000\n",
      "minibatch loss 0.316408\n",
      "minibatch accuracy 0.960938\n",
      "validation accuracy 0.888976\n",
      "step 80000\n",
      "minibatch loss 0.411959\n",
      "minibatch accuracy 0.898438\n",
      "validation accuracy 0.888752\n",
      "step 85000\n",
      "minibatch loss 0.684641\n",
      "minibatch accuracy 0.820312\n",
      "validation accuracy 0.889424\n",
      "step 90000\n",
      "minibatch loss 0.422998\n",
      "minibatch accuracy 0.906250\n",
      "validation accuracy 0.889536\n",
      "step 95000\n",
      "minibatch loss 0.455742\n",
      "minibatch accuracy 0.882812\n",
      "validation accuracy 0.892449\n",
      "step 100000\n",
      "minibatch loss 0.510654\n",
      "minibatch accuracy 0.867188\n",
      "validation accuracy 0.890993\n",
      "0.946682\n"
     ]
    }
   ],
   "source": [
    "#fully connected NN + L2 + dropout\n",
    "batch_size = 128\n",
    "num_steps = 100000 + 1\n",
    "beta = 0.001\n",
    "\n",
    "#input data and labels\n",
    "x = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "#hidden layer\n",
    "W_1 = tf.Variable(tf.truncated_normal([image_size * image_size, 1024]))\n",
    "b_1 = tf.Variable(tf.zeros([1024]))\n",
    "h_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_1_drop = tf.nn.dropout(h_1, keep_prob)\n",
    "\n",
    "#output layer\n",
    "W_2 = tf.Variable(tf.truncated_normal([1024, num_labels]))\n",
    "b_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "logits = tf.matmul(h_1_drop, W_2) + b_2\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_true))\n",
    "l2 = beta * (tf.reduce_mean(tf.nn.l2_loss(W_1)) + tf.reduce_mean(tf.nn.l2_loss(W_2)))\n",
    "loss = loss + l2\n",
    "\n",
    "#optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "#accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(prob, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        _, l, a = sess.run([optimizer, loss, accuracy], feed_dict={x: batch_data, \n",
    "                                                                   y_true: batch_labels, \n",
    "                                                                   keep_prob: 0.5})\n",
    "        if step % 5000 == 0:\n",
    "            print(\"step %d\" % step)\n",
    "            print(\"minibatch loss %f\" % l)\n",
    "            print(\"minibatch accuracy %f\" % a)\n",
    "            a = sess.run(accuracy, feed_dict={x: valid_dataset, y_true: valid_labels, keep_prob: 1.0})\n",
    "            print(\"validation accuracy %f\" % a)\n",
    "        \n",
    "    print(sess.run(accuracy, feed_dict={x: test_dataset, y_true: test_labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "minibatch loss 802.539185\n",
      "minibatch accuracy 0.070312\n",
      "validation accuracy 0.266525\n",
      "step 5000\n",
      "minibatch loss 115.885597\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.695048\n",
      "step 10000\n",
      "minibatch loss 42.680996\n",
      "minibatch accuracy 0.992188\n",
      "validation accuracy 0.695832\n",
      "step 15000\n",
      "minibatch loss 15.681966\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.704347\n",
      "step 20000\n",
      "minibatch loss 5.768995\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.700202\n",
      "step 25000\n",
      "minibatch loss 2.128421\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.705243\n",
      "step 30000\n",
      "minibatch loss 0.801452\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.709724\n",
      "step 35000\n",
      "minibatch loss 0.315725\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.712973\n",
      "step 40000\n",
      "minibatch loss 0.136453\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.713646\n",
      "step 45000\n",
      "minibatch loss 0.070772\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.712973\n",
      "step 50000\n",
      "minibatch loss 0.046814\n",
      "minibatch accuracy 1.000000\n",
      "validation accuracy 0.713198\n"
     ]
    }
   ],
   "source": [
    "batch_data = train_dataset[:batch_size, :]\n",
    "batch_labels = train_labels[:batch_size, :]\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "\n",
    "        _, l, a = sess.run([optimizer, loss, accuracy], feed_dict={x: batch_data, \n",
    "                                                                   y_true: batch_labels, \n",
    "                                                                   keep_prob: 0.5})\n",
    "        if step % 5000 == 0:\n",
    "            print(\"step %d\" % step)\n",
    "            print(\"minibatch loss %f\" % l)\n",
    "            print(\"minibatch accuracy %f\" % a)\n",
    "            a = sess.run(accuracy, feed_dict={x: valid_dataset, y_true: valid_labels, keep_prob: 1.0})\n",
    "            print(\"validation accuracy %f\" % a)\n",
    "        \n",
    "    print(sess.run(accuracy, feed_dict={x: test_dataset, y_true: test_labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 ,learning rate: 0.0001\n",
      "validation accuracy 0.815147\n",
      "epoch: 2 ,learning rate: 9.5e-05\n",
      "validation accuracy 0.824669\n",
      "epoch: 3 ,learning rate: 9.025e-05\n",
      "validation accuracy 0.8324\n",
      "epoch: 4 ,learning rate: 8.57375e-05\n",
      "validation accuracy 0.835089\n",
      "epoch: 5 ,learning rate: 8.1450625e-05\n",
      "validation accuracy 0.837665\n",
      "epoch: 6 ,learning rate: 7.737809375e-05\n",
      "validation accuracy 0.842371\n",
      "epoch: 7 ,learning rate: 7.35091890625e-05\n",
      "validation accuracy 0.842035\n",
      "epoch: 8 ,learning rate: 6.98337296094e-05\n",
      "validation accuracy 0.844163\n",
      "epoch: 9 ,learning rate: 6.63420431289e-05\n",
      "validation accuracy 0.847076\n",
      "epoch: 10 ,learning rate: 6.30249409725e-05\n",
      "validation accuracy 0.847524\n",
      "epoch: 11 ,learning rate: 5.98736939238e-05\n",
      "validation accuracy 0.849205\n",
      "epoch: 12 ,learning rate: 5.68800092276e-05\n",
      "validation accuracy 0.848308\n",
      "epoch: 13 ,learning rate: 5.40360087663e-05\n",
      "validation accuracy 0.84842\n",
      "epoch: 14 ,learning rate: 5.1334208328e-05\n",
      "validation accuracy 0.849429\n",
      "epoch: 15 ,learning rate: 4.87674979116e-05\n",
      "validation accuracy 0.849877\n",
      "epoch: 16 ,learning rate: 4.6329123016e-05\n",
      "validation accuracy 0.849989\n",
      "epoch: 17 ,learning rate: 4.40126668652e-05\n",
      "validation accuracy 0.851669\n",
      "epoch: 18 ,learning rate: 4.18120335219e-05\n",
      "validation accuracy 0.851221\n",
      "epoch: 19 ,learning rate: 3.97214318458e-05\n",
      "validation accuracy 0.851333\n",
      "epoch: 20 ,learning rate: 3.77353602535e-05\n",
      "validation accuracy 0.85223\n",
      "test accuracy 0.919557\n"
     ]
    }
   ],
   "source": [
    "#fully connected NN + L2\n",
    "batch_size = 100\n",
    "n_hidden = 4096\n",
    "n_epoch = 20\n",
    "beta = 0.001\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "#input data and labels\n",
    "x = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "#hidden layer1\n",
    "W_1 = tf.Variable(tf.random_normal([image_size * image_size, n_hidden]))\n",
    "b_1 = tf.Variable(tf.random_normal([n_hidden]))\n",
    "h_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "h_1_drop = tf.nn.dropout(h_1, keep_prob)\n",
    "\n",
    "#hidden layer2\n",
    "W_2 = tf.Variable(tf.random_normal([n_hidden, n_hidden]))\n",
    "b_2 = tf.Variable(tf.random_normal([n_hidden]))\n",
    "h_2 = tf.nn.relu(tf.matmul(h_1_drop, W_2) + b_2)\n",
    "h_2_drop = tf.nn.dropout(h_2, keep_prob)\n",
    "\n",
    "#output layer\n",
    "W_3 = tf.Variable(tf.random_normal([n_hidden, num_labels]))\n",
    "b_3 = tf.Variable(tf.random_normal([num_labels]))\n",
    "logits = tf.matmul(h_2_drop, W_3) + b_3\n",
    "prob = tf.nn.softmax(logits)\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_true))\n",
    "l2 = tf.reduce_mean(tf.nn.l2_loss(W_1)) + tf.reduce_mean(tf.nn.l2_loss(W_2)) + tf.reduce_mean(tf.nn.l2_loss(W_3))\n",
    "loss += beta * l2\n",
    "\n",
    "#accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(prob, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#training\n",
    "lr = tf.placeholder(\"float\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)\n",
    "# optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    lr_val = 1e-4\n",
    "    tf.initialize_all_variables().run()\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        print(\"epoch:\", epoch, \",learning rate:\", lr_val)\n",
    "        indices = np.random.permutation(train_dataset.shape[0])\n",
    "        for step in range(train_dataset.shape[0] / batch_size):\n",
    "            offset = step * batch_size\n",
    "            batch_indices = indices[offset:(offset + batch_size)]\n",
    "            batch_data = train_dataset[batch_indices, :]\n",
    "            batch_labels = train_labels[batch_indices, :]\n",
    "            sess.run(optimizer, feed_dict={x: batch_data,\n",
    "                                           y_true: batch_labels,\n",
    "                                           keep_prob: 0.5,\n",
    "                                           lr: lr_val})\n",
    "#         print(\"train accuracy\", \n",
    "#              sess.run(accuracy, feed_dict={x: train_dataset, y_true: train_labels, keep_prob: 1.0}))\n",
    "        print(\"validation accuracy\",\n",
    "              sess.run(accuracy, feed_dict={x: valid_dataset, y_true: valid_labels, keep_prob: 1.0}))\n",
    "        lr_val *= 0.95\n",
    "        \n",
    "    print(\"test accuracy\", \n",
    "          sess.run(accuracy, feed_dict={x: test_dataset, y_true: test_labels, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
